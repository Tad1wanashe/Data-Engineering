{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project is designed to help the United States of America to be able to track and perform analytical queries on the people that enter the country - immigrants.\n",
    "It consists of a star schema with 5 dimensions and one fact table. The United States of America will be able to report on the amount of people entering their country and where they come from, what visa type they are in ie. The Data provided was Big Data so to be able to perform such operations we make use of Apache Spark.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up\n",
    "\n",
    "## Step 1: Scope the project and Gather Data\n",
    "\n",
    "The project that was conducted for this capstone was provided by Udacity. This project allowed the student to have an opportunity to show case their new found skills from the nanodegree. It was used as a final piece of grading.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "pd.set_option('display.width',170, 'display.max_rows',200, 'display.max_columns',900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "I plan on providing this solution to Immigration in the United States of America. Due to the \"American Dream\" America finds it self in a position where it has a lot of people enter the country. This would help them perform better analytical queries. \n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "* airport-codes_csv.csv - Is a table of airport codes and full location details of where the airports are and coordinates\n",
    "\n",
    "* immigration_data_sample.csv - This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. This is where the data comes from. \n",
    "\n",
    "* U.S. City Demographic Data: This data comes from OpenSoft. You can read more about it here.\n",
    "\n",
    "These two methods allow for better analysis of how our data is stored.\n",
    "They provide us with a wide variety of information from just a plain view of what the data looks like\n",
    "to how many unique values we have in the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "def Source_information(df):\n",
    "    print (\"\\n\\n---------------------\")\n",
    "    print (\"Dataset INFORMATION\")\n",
    "    print (\"---------------------\")\n",
    "    print (\"Shape of data set:\", df.shape, \"\\n\")\n",
    "    print (\"Column Headers:\", list(df.columns.values), \"\\n\")\n",
    "    print (df.dtypes)\n",
    "    \n",
    "def Source_Full_report(df):\n",
    "    import re\n",
    "    missing_values = []\n",
    "    nonumeric_values = []\n",
    "\n",
    "    print (\"Dataset INFORMATION\")\n",
    "    print (\"========================\\n\")\n",
    "\n",
    "    for column in df:\n",
    "        # Find all the unique feature values\n",
    "        uniq = df[column].unique()\n",
    "        print (\"'{}' has {} unique values\" .format(column,uniq.size))\n",
    "        if (uniq.size > 10):\n",
    "            print(\"~~Listing up to 10 unique values~~\")\n",
    "        print (uniq[0:10])\n",
    "        print (\"\\n-----------------------------------------------------------------------\\n\")\n",
    "\n",
    "        # Find features with missing values\n",
    "        if (True in pd.isnull(uniq)):\n",
    "            s = \"{} has {} missing\" .format(column, pd.isnull(df[column]).sum())\n",
    "            missing_values.append(s)\n",
    "\n",
    "        # Find features with non-numeric values\n",
    "        for i in range (1, np.prod(uniq.shape)):\n",
    "            if (re.match('nan', str(uniq[i]))):\n",
    "                break\n",
    "            if not (re.search('(^\\d+\\.?\\d*$)|(^\\d*\\.?\\d+$)', str(uniq[i]))):\n",
    "                nonumeric_values.append(column)\n",
    "                break\n",
    "\n",
    "    print (\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "    print (\"Features with missing values:\\n{}\\n\\n\" .format(missing_values))\n",
    "    print (\"Features with non-numeric values:\\n{}\" .format(nonumeric_values))\n",
    "    print (\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "###The below allows us to view the data that is stored in the sas_data folder using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "folder = 'sas_data'\n",
    "df = pq.ParquetDataset(folder).read_pandas().to_pandas(split_blocks=True, self_destruct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------------------\n",
      "Dataset INFORMATION\n",
      "---------------------\n",
      "Shape of data set: (3096313, 28) \n",
      "\n",
      "Column Headers: ['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port', 'arrdate', 'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa', 'count', 'dtadfile', 'visapost', 'occup', 'entdepa', 'entdepd', 'entdepu', 'matflag', 'biryear', 'dtaddto', 'gender', 'insnum', 'airline', 'admnum', 'fltno', 'visatype'] \n",
      "\n",
      "cicid       float64\n",
      "i94yr       float64\n",
      "i94mon      float64\n",
      "i94cit      float64\n",
      "i94res      float64\n",
      "i94port      object\n",
      "arrdate     float64\n",
      "i94mode     float64\n",
      "i94addr      object\n",
      "depdate     float64\n",
      "i94bir      float64\n",
      "i94visa     float64\n",
      "count       float64\n",
      "dtadfile     object\n",
      "visapost     object\n",
      "occup        object\n",
      "entdepa      object\n",
      "entdepd      object\n",
      "entdepu      object\n",
      "matflag      object\n",
      "biryear     float64\n",
      "dtaddto      object\n",
      "gender       object\n",
      "insnum       object\n",
      "airline      object\n",
      "admnum      float64\n",
      "fltno        object\n",
      "visatype     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "Source_information(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>biryear</th>\n",
       "      <th>admnum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.096313e+06</td>\n",
       "      <td>3096313.0</td>\n",
       "      <td>3096313.0</td>\n",
       "      <td>3.096313e+06</td>\n",
       "      <td>3.096313e+06</td>\n",
       "      <td>3.096313e+06</td>\n",
       "      <td>3.096074e+06</td>\n",
       "      <td>2.953856e+06</td>\n",
       "      <td>3.095511e+06</td>\n",
       "      <td>3.096313e+06</td>\n",
       "      <td>3096313.0</td>\n",
       "      <td>3.095511e+06</td>\n",
       "      <td>3.096313e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.078652e+06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.049069e+02</td>\n",
       "      <td>3.032838e+02</td>\n",
       "      <td>2.055985e+04</td>\n",
       "      <td>1.073690e+00</td>\n",
       "      <td>2.057395e+04</td>\n",
       "      <td>4.176761e+01</td>\n",
       "      <td>1.845393e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.974232e+03</td>\n",
       "      <td>7.082885e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.763278e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.100269e+02</td>\n",
       "      <td>2.085832e+02</td>\n",
       "      <td>8.777339e+00</td>\n",
       "      <td>5.158963e-01</td>\n",
       "      <td>2.935697e+01</td>\n",
       "      <td>1.742026e+01</td>\n",
       "      <td>3.983910e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.742026e+01</td>\n",
       "      <td>2.215442e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.010000e+02</td>\n",
       "      <td>1.010000e+02</td>\n",
       "      <td>2.054500e+04</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.517600e+04</td>\n",
       "      <td>-3.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.902000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.577790e+06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.350000e+02</td>\n",
       "      <td>1.310000e+02</td>\n",
       "      <td>2.055200e+04</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.056100e+04</td>\n",
       "      <td>3.000000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>5.603523e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.103507e+06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.130000e+02</td>\n",
       "      <td>2.130000e+02</td>\n",
       "      <td>2.056000e+04</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.057000e+04</td>\n",
       "      <td>4.100000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.975000e+03</td>\n",
       "      <td>5.936094e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.654341e+06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.120000e+02</td>\n",
       "      <td>5.040000e+02</td>\n",
       "      <td>2.056700e+04</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.057900e+04</td>\n",
       "      <td>5.400000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.986000e+03</td>\n",
       "      <td>9.350987e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.102785e+06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.990000e+02</td>\n",
       "      <td>7.600000e+02</td>\n",
       "      <td>2.057400e+04</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>4.542700e+04</td>\n",
       "      <td>1.140000e+02</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.019000e+03</td>\n",
       "      <td>9.991557e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              cicid      i94yr     i94mon        i94cit        i94res       arrdate       i94mode       depdate        i94bir       i94visa      count       biryear  \\\n",
       "count  3.096313e+06  3096313.0  3096313.0  3.096313e+06  3.096313e+06  3.096313e+06  3.096074e+06  2.953856e+06  3.095511e+06  3.096313e+06  3096313.0  3.095511e+06   \n",
       "mean   3.078652e+06     2016.0        4.0  3.049069e+02  3.032838e+02  2.055985e+04  1.073690e+00  2.057395e+04  4.176761e+01  1.845393e+00        1.0  1.974232e+03   \n",
       "std    1.763278e+06        0.0        0.0  2.100269e+02  2.085832e+02  8.777339e+00  5.158963e-01  2.935697e+01  1.742026e+01  3.983910e-01        0.0  1.742026e+01   \n",
       "min    6.000000e+00     2016.0        4.0  1.010000e+02  1.010000e+02  2.054500e+04  1.000000e+00  1.517600e+04 -3.000000e+00  1.000000e+00        1.0  1.902000e+03   \n",
       "25%    1.577790e+06     2016.0        4.0  1.350000e+02  1.310000e+02  2.055200e+04  1.000000e+00  2.056100e+04  3.000000e+01  2.000000e+00        1.0  1.962000e+03   \n",
       "50%    3.103507e+06     2016.0        4.0  2.130000e+02  2.130000e+02  2.056000e+04  1.000000e+00  2.057000e+04  4.100000e+01  2.000000e+00        1.0  1.975000e+03   \n",
       "75%    4.654341e+06     2016.0        4.0  5.120000e+02  5.040000e+02  2.056700e+04  1.000000e+00  2.057900e+04  5.400000e+01  2.000000e+00        1.0  1.986000e+03   \n",
       "max    6.102785e+06     2016.0        4.0  9.990000e+02  7.600000e+02  2.057400e+04  9.000000e+00  4.542700e+04  1.140000e+02  3.000000e+00        1.0  2.019000e+03   \n",
       "\n",
       "             admnum  \n",
       "count  3.096313e+06  \n",
       "mean   7.082885e+10  \n",
       "std    2.215442e+10  \n",
       "min    0.000000e+00  \n",
       "25%    5.603523e+10  \n",
       "50%    5.936094e+10  \n",
       "75%    9.350987e+10  \n",
       "max    9.991557e+10  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset INFORMATION\n",
      "========================\n",
      "\n",
      "'cicid' has 3096313 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[ 6.  7. 15. 16. 17. 18. 19. 20. 21. 22.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'i94yr' has 1 unique values\n",
      "[2016.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'i94mon' has 1 unique values\n",
      "[4.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'i94cit' has 243 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[692. 254. 101. 102. 103. 104. 105. 107. 108. 109.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'i94res' has 229 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[692. 276. 101. 110. 117. 112. 251. 102. 103. 104.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'i94port' has 299 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['XXX' 'ATL' 'WAS' 'NYC' 'TOR' 'BOS' 'HOU' 'MIA' 'CHI' 'LOS']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'arrdate' has 30 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[20573. 20551. 20545. 20546. 20547. 20548. 20549. 20550. 20552. 20553.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'i94mode' has 5 unique values\n",
      "[nan  1.  2.  9.  3.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'i94addr' has 458 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[None 'AL' 'MI' 'MA' 'NJ' 'NY' 'MO' 'TX' 'CT' 'FL']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'depdate' has 236 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[   nan 20691. 20567. 20555. 20558. 20553. 20562. 20671. 20554. 20549.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'i94bir' has 113 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[37. 25. 55. 28.  4. 57. 63. 46. 48. 52.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'i94visa' has 3 unique values\n",
      "[2. 3. 1.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'count' has 1 unique values\n",
      "[1.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'dtadfile' has 118 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[None '20130811' '20160401' '20160402' '20160403' '20160404' '20160405'\n",
      " '20160406' '20160407' '20160408']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'visapost' has 531 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[None 'SEO' 'TIA' 'HLS' 'FLR' 'NPL' 'RME' 'BRL' 'FRN' 'TLV']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'occup' has 112 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[None 'ELT' 'PHS' 'EXA' 'STU' 'MKT' 'TIP' 'ULS' 'OTH' 'RPT']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'entdepa' has 14 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['T' 'G' 'O' 'H' 'U' 'B' 'K' 'M' 'F' None]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'entdepd' has 13 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[None 'O' 'K' 'I' 'Q' 'R' 'N' 'M' 'J' 'D']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'entdepu' has 3 unique values\n",
      "['U' 'Y' None]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'matflag' has 2 unique values\n",
      "[None 'M']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'biryear' has 113 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[1979. 1991. 1961. 1988. 2012. 1959. 1953. 1970. 1968. 1964.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'dtaddto' has 778 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['10282016' 'D/S' '09302016' '04062016' '06292016' '03312018' '05202016'\n",
      " '10012016' '06262016' '06282016']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'gender' has 5 unique values\n",
      "[None 'M' 'F' 'X' 'U']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'insnum' has 1914 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[None '3181' '3148' '2295' '3511' '3451' '3993' '4171' '6758' '6962']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'airline' has 535 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[None 'OS' 'AA' 'AZ' 'TK' 'MQ' 'LH' 'AY' 'AB' 'AF']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'admnum' has 3075579 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[1.89762848e+09 3.73679633e+09 6.66643185e+08 9.24684613e+10\n",
      " 9.24684631e+10 9.24710380e+10 9.24713992e+10 9.24716138e+10\n",
      " 9.24707960e+10 9.24784897e+10]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'fltno' has 7153 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[None '00296' '93' '00199' '00602' '00608' '00001' '03348' '00422' '00614']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'visatype' has 17 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['B2' 'F1' 'B1' 'WT' 'WB' 'E2' 'I' 'F2' 'E1' 'M1']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Features with missing values:\n",
      "['i94mode has 239 missing', 'i94addr has 152592 missing', 'depdate has 142457 missing', 'i94bir has 802 missing', 'dtadfile has 1 missing', 'visapost has 1881250 missing', 'occup has 3088187 missing', 'entdepa has 238 missing', 'entdepd has 138429 missing', 'entdepu has 3095921 missing', 'matflag has 138429 missing', 'biryear has 802 missing', 'dtaddto has 477 missing', 'gender has 414269 missing', 'insnum has 2982605 missing', 'airline has 83627 missing', 'fltno has 19549 missing']\n",
      "\n",
      "\n",
      "Features with non-numeric values:\n",
      "['i94port', 'i94addr', 'visapost', 'occup', 'entdepa', 'entdepd', 'entdepu', 'matflag', 'dtaddto', 'gender', 'insnum', 'airline', 'fltno', 'visatype']\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Source_Full_report(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "There is a chance some of our columns are empty due to the large volumes of data that we are dealing with.\n",
    "I have used the below to get a percentage of how much is missing in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cicid        0.000000\n",
       "i94yr        0.000000\n",
       "i94mon       0.000000\n",
       "i94cit       0.000000\n",
       "i94res       0.000000\n",
       "i94port      0.000000\n",
       "arrdate      0.000000\n",
       "i94mode      0.007719\n",
       "i94addr      4.928184\n",
       "depdate      4.600859\n",
       "i94bir       0.025902\n",
       "i94visa      0.000000\n",
       "count        0.000000\n",
       "dtadfile     0.000032\n",
       "visapost    60.757746\n",
       "occup       99.737559\n",
       "entdepa      0.007687\n",
       "entdepd      4.470769\n",
       "entdepu     99.987340\n",
       "matflag      4.470769\n",
       "biryear      0.025902\n",
       "dtaddto      0.015405\n",
       "gender      13.379429\n",
       "insnum      96.327632\n",
       "airline      2.700857\n",
       "admnum       0.000000\n",
       "fltno        0.631364\n",
       "visatype     0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.isnull().sum() / len(df))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Duplicity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df[['cicid','gender', 'i94addr', 'visapost']]\n",
    "size = df1.groupby('cicid')['gender','i94addr'].size().reset_index()\n",
    "size[size[0] > 1]        # DATAFRAME OF DUPLICATES\n",
    "\n",
    "len(size[size[0] > 1])   # NUMBER OF DUPLICATES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Airport Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "airport = pd.read_csv(\"airport-codes_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft continent iso_country iso_region  municipality gps_code iata_code local_code  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0       NaN          US      US-PA      Bensalem      00A       NaN        00A   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0       NaN          US      US-KS         Leoti     00AA       NaN       00AA   \n",
       "2  00AK  small_airport                        Lowell Field         450.0       NaN          US      US-AK  Anchor Point     00AK       NaN       00AK   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0       NaN          US      US-AL       Harvest     00AL       NaN       00AL   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0       NaN          US      US-AR       Newport      NaN       NaN        NaN   \n",
       "\n",
       "                             coordinates  \n",
       "0     -74.93360137939453, 40.07080078125  \n",
       "1                 -101.473911, 38.704022  \n",
       "2            -151.695999146, 59.94919968  \n",
       "3  -86.77030181884766, 34.86479949951172  \n",
       "4                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------------------\n",
      "Dataset INFORMATION\n",
      "---------------------\n",
      "Shape of data set: (55075, 12) \n",
      "\n",
      "Column Headers: ['ident', 'type', 'name', 'elevation_ft', 'continent', 'iso_country', 'iso_region', 'municipality', 'gps_code', 'iata_code', 'local_code', 'coordinates'] \n",
      "\n",
      "ident            object\n",
      "type             object\n",
      "name             object\n",
      "elevation_ft    float64\n",
      "continent        object\n",
      "iso_country      object\n",
      "iso_region       object\n",
      "municipality     object\n",
      "gps_code         object\n",
      "iata_code        object\n",
      "local_code       object\n",
      "coordinates      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "Source_information(airport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset INFORMATION\n",
      "========================\n",
      "\n",
      "'ident' has 55075 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['00A' '00AA' '00AK' '00AL' '00AR' '00AS' '00AZ' '00CA' '00CL' '00CN']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'type' has 7 unique values\n",
      "['heliport' 'small_airport' 'closed' 'seaplane_base' 'balloonport'\n",
      " 'medium_airport' 'large_airport']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'name' has 52144 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['Total Rf Heliport' 'Aero B Ranch Airport' 'Lowell Field' 'Epps Airpark'\n",
      " 'Newport Hospital & Clinic Heliport' 'Fulton Airport' 'Cordes Airport'\n",
      " 'Goldstone /Gts/ Airport' 'Williams Ag Airport'\n",
      " 'Kitchen Creek Helibase Heliport']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'elevation_ft' has 5450 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[  11. 3435.  450.  820.  237. 1100. 3810. 3038.   87. 3350.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'continent' has 7 unique values\n",
      "[nan 'OC' 'AF' 'AN' 'EU' 'AS' 'SA']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'iso_country' has 244 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['US' 'PR' 'MH' 'MP' 'GU' 'SO' 'AQ' 'GB' 'PG' 'AD']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'iso_region' has 2810 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['US-PA' 'US-KS' 'US-AK' 'US-AL' 'US-AR' 'US-OK' 'US-AZ' 'US-CA' 'US-CO'\n",
      " 'US-FL']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'municipality' has 27134 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['Bensalem' 'Leoti' 'Anchor Point' 'Harvest' 'Newport' 'Alex' 'Cordes'\n",
      " 'Barstow' 'Biggs' 'Pine Valley']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'gps_code' has 40851 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['00A' '00AA' '00AK' '00AL' nan '00AS' '00AZ' '00CA' '00CL' '00CN']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'iata_code' has 9043 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[nan 'UTK' 'OCA' 'PQS' 'CSE' 'JCY' 'PMX' 'WLR' 'NUP' 'PTC']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'local_code' has 27437 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['00A' '00AA' '00AK' '00AL' nan '00AS' '00AZ' '00CA' '00CL' '00CN']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'coordinates' has 54874 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['-74.93360137939453, 40.07080078125' '-101.473911, 38.704022'\n",
      " '-151.695999146, 59.94919968' '-86.77030181884766, 34.86479949951172'\n",
      " '-91.254898, 35.6087' '-97.8180194, 34.9428028'\n",
      " '-112.16500091552734, 34.305599212646484'\n",
      " '-116.888000488, 35.350498199499995' '-121.763427, 39.427188'\n",
      " '-116.4597417, 32.7273736']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Features with missing values:\n",
      "['elevation_ft has 7006 missing', 'continent has 27719 missing', 'iso_country has 247 missing', 'municipality has 5676 missing', 'gps_code has 14045 missing', 'iata_code has 45886 missing', 'local_code has 26389 missing']\n",
      "\n",
      "\n",
      "Features with non-numeric values:\n",
      "['ident', 'type', 'name', 'continent', 'iso_country', 'iso_region', 'municipality', 'gps_code', 'iata_code', 'local_code', 'coordinates']\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Source_Full_report(airport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_file_name = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "Global_temp = pd.read_csv(temp_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  Country Latitude Longitude\n",
       "0  1743-11-01               6.068                          1.737  Århus  Denmark   57.05N    10.33E\n",
       "1  1743-12-01                 NaN                            NaN  Århus  Denmark   57.05N    10.33E\n",
       "2  1744-01-01                 NaN                            NaN  Århus  Denmark   57.05N    10.33E\n",
       "3  1744-02-01                 NaN                            NaN  Århus  Denmark   57.05N    10.33E\n",
       "4  1744-03-01                 NaN                            NaN  Århus  Denmark   57.05N    10.33E"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Global_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographic = pd.read_csv(\"us-cities-demographics.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  Female Population  Total Population  Number of Veterans  Foreign-born  Average Household Size  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0            41862.0             82463              1562.0       30908.0                    2.60   \n",
       "1            Quincy  Massachusetts        41.0          44129.0            49500.0             93629              4147.0       32935.0                    2.39   \n",
       "2            Hoover        Alabama        38.5          38040.0            46799.0             84839              4819.0        8229.0                    2.58   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0            87105.0            175232              5821.0       33878.0                    3.18   \n",
       "4            Newark     New Jersey        34.6         138040.0           143873.0            281913              5829.0       86253.0                    2.73   \n",
       "\n",
       "  State Code                       Race  Count  \n",
       "0         MD         Hispanic or Latino  25924  \n",
       "1         MA                      White  58723  \n",
       "2         AL                      Asian   4759  \n",
       "3         CA  Black or African-American  24437  \n",
       "4         NJ                      White  76402  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------------------\n",
      "Dataset INFORMATION\n",
      "---------------------\n",
      "Shape of data set: (2891, 12) \n",
      "\n",
      "Column Headers: ['City', 'State', 'Median Age', 'Male Population', 'Female Population', 'Total Population', 'Number of Veterans', 'Foreign-born', 'Average Household Size', 'State Code', 'Race', 'Count'] \n",
      "\n",
      "City                       object\n",
      "State                      object\n",
      "Median Age                float64\n",
      "Male Population           float64\n",
      "Female Population         float64\n",
      "Total Population            int64\n",
      "Number of Veterans        float64\n",
      "Foreign-born              float64\n",
      "Average Household Size    float64\n",
      "State Code                 object\n",
      "Race                       object\n",
      "Count                       int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "Source_information(demographic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset INFORMATION\n",
      "========================\n",
      "\n",
      "'City' has 567 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['Silver Spring' 'Quincy' 'Hoover' 'Rancho Cucamonga' 'Newark' 'Peoria'\n",
      " 'Avondale' 'West Covina' \"O'Fallon\" 'High Point']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'State' has 49 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['Maryland' 'Massachusetts' 'Alabama' 'California' 'New Jersey' 'Illinois'\n",
      " 'Arizona' 'Missouri' 'North Carolina' 'Pennsylvania']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'Median Age' has 180 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[33.8 41.  38.5 34.5 34.6 33.1 29.1 39.8 36.  35.5]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'Male Population' has 594 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[ 40601.  44129.  38040.  88127. 138040.  56229.  38712.  51629.  41762.\n",
      "  51751.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'Female Population' has 595 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[ 41862.  49500.  46799.  87105. 143873.  62432.  41971.  56860.  43270.\n",
      "  58077.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'Total Population' has 594 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[ 82463  93629  84839 175232 281913 118661  80683 108489  85032 109828]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'Number of Veterans' has 578 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[1562. 4147. 4819. 5821. 5829. 6634. 4815. 3800. 5783. 5204.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'Foreign-born' has 588 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[30908. 32935.  8229. 33878. 86253.  7517.  8355. 37038.  3269. 16315.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'Average Household Size' has 162 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[2.6  2.39 2.58 3.18 2.73 2.4  3.56 2.77 2.65 2.62]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'State Code' has 49 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['MD' 'MA' 'AL' 'CA' 'NJ' 'IL' 'AZ' 'MO' 'NC' 'PA']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'Race' has 5 unique values\n",
      "['Hispanic or Latino' 'White' 'Asian' 'Black or African-American'\n",
      " 'American Indian and Alaska Native']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'Count' has 2785 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[25924 58723  4759 24437 76402  1343 11592 32716  2583 11060]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Features with missing values:\n",
      "['Male Population has 3 missing', 'Female Population has 3 missing', 'Number of Veterans has 13 missing', 'Foreign-born has 13 missing', 'Average Household Size has 16 missing']\n",
      "\n",
      "\n",
      "Features with non-numeric values:\n",
      "['City', 'State', 'State Code', 'Race']\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Source_Full_report(demographic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\t\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "df_spark.write.parquet(\"sas_data\")\n",
    "df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "This is found in the USA IMMIGRANT STAR SCHEMA image.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This is found in the PySpark Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This can also be found in the PySpark notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "This can be found in the DataDictionary.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people.\n",
    " \n",
    " * If the data was increased by 100x I would make use of Amazon EMR because:\n",
    " For cost-effective and quick performance of data transformation workloads (ETL) like sort, join and aggregate on large datasets, you can use EMR. \n",
    "* I would use that in conjuction with Apache Airflow to schedule when certain analytics need to be done and multiple users need to be notified of the success or failures etc.\n",
    "* If this report is looked at everyday It should be run day but I suspect it will only be needed once a month so I would schedule it to run at the beginning of every month."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
